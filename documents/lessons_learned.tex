%!TeX root = lessons_learned
\documentclass[paper.tex]{subfiles}
\begin{document}


\paragraph{Lessons Learned} \

\rule{\linewidth}{0.2pt}

It is interesting to think about the meta-issues surrounding this project. Beta-glo assay, not a few kilometers away the food bank ran out of food and people were not able. 

\rule{\linewidth}{0.2pt}

This one was taught to me by Yuriy!

We were trying to write a pre-baked math worksheet into Wolfram Mathematica.

It is useful to first strictly implement precisely as written, and only when all parts are operational and understood, begin modifying for your own use.

This is common practice in many fields, but hadn't occurred to me for math. 

\rule{\linewidth}{0.2pt}

Indexes of papers compiled by government agencies, and gray literature in general, appear to be very useful.

\rule{\linewidth}{0.2pt}

Rigorous literature reviews are very important. de Seze has a good example: search terms are specified. If we had come across the paper by Bigior earlier, we may have

\rule{\linewidth}{0.2pt}

Papers only carry the scientific heritage that their authors saw fit to include in the references section, and only provide the context included in the "introduction". 

\rule{\linewidth}{0.2pt}

There seems to be a drive towards small volumes and microfluidics. This is great; these are cool technologies; but in our case these were a huge hindrance.

There are a lot of reasons why small volumes are useful: you can run more tests with a given quantity of sample - a 1500-well microplate assay would be a hard sell with even 1.5 mL tubes; small samples are diluted to a lesser degree. 

However, it should also be considered that most tolerances and precisions do not scale with volume. 
Rheology does not scale with volume, so bacterial culture becomes a challenge. Dust and contaminants do not scale. Optical path lengths are much shorter. Most assays will have decreased resolution. Precision in dispensing is much more critical, as is deformation due to autoclaving. The "hairs" that appear on machined polypropylene, which are almost impossible to remove, can be as large as your sample. Evaporation is much more critical.

Most importantly, annoyance scales inversely with size. It is hard to work with a quantity of fluid which is barely visible.

\rule{\linewidth}{0.2pt}

We often came across a situation in which minor errors in software,  would completely change the conclusion of our results.

Reproducible coding helps, but it would be much better if we had used standard software engineering best practices; red-teaming, unit testing on randomly generated data, etc. It might also be helpful if the data itself are blinded against randomly generated controls - in many cases that's not possible, but it might be useful in some circumstances.

See Phosphine.

It may be useful to make explicit to the reader where particular sensitivities - high gradients of conclusion against some factor - might exist in your research.

\rule{\linewidth}{0.2pt}

The phage field - and many other fields - seem to have a problem where most of the really important base discoveries related to practice - 

Some things actually seem to have been lost.

\rule{\linewidth}{0.2pt}

Writing a paper by writing the chapters and headings that you want, and then filling in the text, is quite hard. Just writing a big chunk of text, and then breaking that into many smaller sections, works a bit better.


\rule{\linewidth}{0.2pt}

It might be useful to watch file changes and autoreload. Practically every program in this project required the same three keystrokes to recompile; a recipe for RSI.

\rule{\linewidth}{0.2pt}

Automatic, no-effort documentation of simulation and experiment runs is critical.

It may be helpful to think of simulations very similarly to IRL experiments. 

A simulation is just a universe in a bottle that you can examine more closely; as with real-world, you can learn much from observation, but 

But this need for documentation conflicts with the rapid, iterative cycle necessary for productivity.

This is obvious to all competent, but when rapidly iteratively testing with simulations, it may be helpful to automatically save a package with images of all the components (schematics, graphs, input files) of each distinct test. 

For comparison to experiment, having a webcam take an image of the assembled board is also helpful. 

Many months of testing have vanished into the ether due to 

Version control alone isn't quite enough. Just having a simulation setup file somewhere in the commit history isn't "discoverable" - that is, you must be able to see what the input and output was without re-running the simulation. 

Manually taking notes tended to disrupt the flow of testing; and in any case, just noting "SIR filter has appropriate phase response" is almost useless. What {\it was} the phase response? Plot it, and upload it!

Software such as Sumatra, Sacred, recipy, and others. In our case, we used eLabFTW's elabapy bindings.

I think this is a good general rule: record as much detail as automatically as possible

\rule{\linewidth}{0.2pt}

It is far easier to use existing, well-characterized reference designs verbatim than to modify 

\rule{\linewidth}{0.2pt}

Dill

When rapidly testing data analyses of simulations which aren't really conducive to unit testing, I previously used to run the whole simulation, look at the analysis, re-run the sim, etc.

This is slow. I often don't change the input parameters for the sim, but just the analysis. However, there's often a large amount of state to persist to disk, so it's not worth writing a serializer just to store this throwaway run.

In this case, saving the entire simulation session with 'dill' is very helpful.

\rule{\linewidth}{0.2pt}

Software opacity is evil.

\rule{\linewidth}{0.2pt}

A great deal of time was spent trying to resolve version conflicts and dependency hells with the numerous libraries used by all the simulation programs. Over a week was spent trying to recursively track down all the This also wastes developer time - some fraction of issues raised are due to library version conflicts. 

Packaged binaries help this slightly, but of course don't help if modifications are required, and managing shared libraries is still a tricky matter.

Good solutions include OpenFOAM's Docker installation. In some cases, using chroot with the original developer's Linux distribution is also of some utility.

But this all seems like quite a lot of overhead and opacity for what ultimately doesn't seem a super-complex problem: deterministically obtain a known-good version of a library, build locally, and set paths appropriately.

A good example might be gTest's cmake integration.

In the extreme, systems exist to extract every 

In some situations (especially where the library has a permissive licence) perhaps it could be useful to consider packaging a complete, batteries-included 'known good' repository, either with the source of all the correct library versions included, or with a script to clone and compile the specific version used, integrating all the libraries with the build system. 

For instance, this was done with the PDB reader in the OpenMM wrapper and JAMES.

Honestly, this might already account for a difference in productivity between, say, Python, and C++. the package manager.

\rule{\linewidth}{0.2pt}

There's also a very neat thing that seems to be common in computational biology, but which doesn't seem popular in other fields. After an algorithm or software tool is written (and the source published separately), a simple CGI frontend is written around it and hosted on the university's servers. eLNemo and the CHARMMing web interface are advanced examples of this. This way, anyone with an input file can get results without futzing around with installation, at least for the few decades that the servers are maintained.

The computational biologists have beat us at our own game.

\rule{\linewidth}{0.2pt}

We have always encountered wasting extreme amounts of time on subtle assembly mistakes in hardware prototypes. 

In one example in this project, many hours were wasted because the enamel insulation on a bodge wire had not burned off completely in a solder joint, leading to a high-impedance connection.

The same has occurred in previous projects; in one example, many days were spent debugging software to fix an apparently slow hardware interrupt, which ended up being the result of a poor solder paste stencil leading to a hidden high-impedance connection to a leadless package.

Many failures are the result of carelessness in modification and lack of inspection. Others would only have been found by a 100\% electrical test.

If flying-probe or bed-of-nails tests can be made sufficiently rapid and closely coupled with the existing toolchain, 

In {\it 2001: A space odyssey}, an automated system is shown guiding the troubleshooting of an assembly, apparently generating a fault tree of all the 

Boundary-scan features might really help 

\rule{\linewidth}{0.2pt}

Numerology doesn't pay

\rule{\linewidth}{0.2pt}

The ability to almost immediately compare simulation to experiment was of great use almost everywhere in this project. This hybrid, "lockstep" approach between theory, simulation, and practice works, I think, extremely well. Even if all these intermediate experiments fail, the mere process of setting up an experiment offers you insight - and forces you to persue leads - which would not otherwise have been gained. Is it better than, say, spending a year carefully establishing the theory behind an experiment, and then doing one? I'm not sure. Other teams have had great success with this.

\rule{\linewidth}{0.2pt}

This project started at the tail end of a project to produce ceramics. Many weeks were wasted; I had convinced myself that ceramics were required over FR4. 

\rule{\linewidth}{0.2pt}

I originally rationalized not contacting others about . There are some 10,000 labs working on a cure; it's not helpful to have random amateurs tying up expert time with ignorant flights of fancy. 

Later, as the project became more mature and seemed likely, this morphed into a sort of arrogance.

An expert in microwave engineering could probably have completed this entire project in a few days, rather than the months it took us.

\rule{\linewidth}{0.2pt}

\paragraph{Hall of Hubris} \

Lest our hats stop fitting

\rule{\linewidth}{0.2pt}

An inane remark:

\begin{displayquote}
We believe once we have the P. Syringae host, we from environmental samples
\end{displayquote}

Truly the depths of Dunning-Kruger.

\rule{\linewidth}{0.2pt}


Lesson learned: implementing safeties in hardware when possible, even in non-critical systems, can greatly speed development.

Like many microwave semiconductor HBT or pHEMT MMICs, the HMC996 amplifier requires.

\rule{\linewidth}{0.2pt}


\rule{\linewidth}{0.2pt}

Sometimes vertical integration is the wrong move. Objectively, in this case, it was simply the *wrong decision*. 

\rule{\linewidth}{0.2pt}


An expert and distinguished gentleman that we contacted regarding assistance resolving transients in our microstrip VCO stated the following:



This was a perfectly sensible remark; it is almost always the case that (at X-band, no less!) a custom IC would have been needed to build a VCO.

Also, if taken in the context of a tired, overworked PI getting an unsolicited email from an excessively verbose undergraduate at a different university, I hardly think I would have replied differently.

However, I think this person may have missed out. Learning 

So perhaps it is wise to ponder the ideas of fools for skeet; one always learns target practice, if only an example what not to do, and occasionally one learns positively. I have often found that I learn greatly from working on the projects of others.

We present this only as a cautionary tale in the hopes that someday I will listen.

 This is 

but, some months later, it was found that he was *right* - absolutely right. This was a grave error in management!

\rule{\linewidth}{0.2pt}

%\end{multicols}

\end{document}