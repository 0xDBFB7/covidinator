%!TeX root = lessons_learned
\documentclass[paper.tex]{subfiles}
\begin{document}


\section{Lessons Learned} \

Made a bonehead mistake while installing lanl-ansi fork of Ipopt. While screwing around in the very beginning, I had accidentally added to -L to the LDFLAGS environment variable. This completely broke the installation. I spent almost 4 hours trying to figure that out. I suppose having a list of commands would perhaps improve things...


LLIS

write in the highest-level language first , then translate. 


the os community has figured out that investment in open work comes back; 

is there a way to "sign" gel images (maybe from the gel-doc itself?) so that manipulation is obvious?


an interesting thing is searchability. I often don't come back to notes because it's easier and more likely I'll get a relevant result by googling again! future lab notebook should have a live and very effective search.

Why do problems keep happening? Why do bugs which other people have hit keep recurring? Why does bad science even happen?
laser power supply troubles paper 


when you build a project, put aside a certain amount of money and time to justify helping future people 


At least with mathematics, it seems to generally be easier to find a starting point closer to your desired one than to work it out yourself. If you get stuck trying to massage things into a different representation, look harder for a different formulation.

\rule{\linewidth}{0.2pt}

In general, even though thesis works are generally pretty high quality, I don't find myself citing them very often. This might be because are packaged to be relevant when searched - breaking a large project with different components into a series of papers probably makes it more useful for later readers.

\rule{\linewidth}{0.2pt}

Peer review is pretty helpful. external input can dissuade you from the apparently frivolous but essential deviations that seem pretty helpful, but in general 

\rule{\linewidth}{0.2pt}

This was written in 4 parts: first, rough lab notes were made, then this was cleaned up into a long thesis-like document, and then this was compressed into the final paper. However, in that final compression step, a lot of "texture" and pedagogy was lost on the first draft, when writing as terse as possible. 

Don't worry about repeating yourself; duplicating sections in different contexts is fine.

\rule{\linewidth}{0.2pt}

Rather than focus effort on making marginal improvements to avoid errors in a certain technique, it may be better to put more effort to find a better experimental technique that is totally immune to these problems. Obviously this isn't always possible or practical.


\rule{\linewidth}{0.2pt}


Lesson learned: implementing safeties in hardware when possible, even in non-critical systems, can greatly speed development.

Like many microwave semiconductor HBT or pHEMT MMICs, the HMC996 amplifier requires careful biasing (discussed in section blah), or else it destroys itself. 






\rule{\linewidth}{0.2pt}


There is something of an over-arching lesson learned - maybe it's not actually good to learn this lesson, maybe it's more useful to assume, but...

It is interesting to think about the meta-issues surrounding this project. While an unusable \$600 Beta-Glo assay went flat, not a few kilometers away the food bank ran out of supplies and people were not able to feed their families. That is unjust. 

On the other hand, we have seen time and again that investment in science and technology - even basic research, so distant from the mechanics of society - can produce outsized effects on quality-of-life. Both NASA and CERN, neither of which do much related to people, produce 7-to-1 and 3-to-1 returns on investment. 

All of which will be a great comfort to the dead. 

You could fit a pandemic through a good-sized breakthrough. The vaccines are an excellent example. Moreover, on a personal level you learn new skills and techniques, to be more useful to others in the future.

So what went wrong here? 

Is it the avenue of investigation? An arrogance in decision making? Perhaps a lack of peer input. Maybe there were too few people involved - now that science is "bigger", we need bigger groups to efficiently manage it? 

Maybe if you don't have sufficient resources, equipment, knowledge, experience, or peers to persue something efficiently, or competently, and developing it slowly would not aid greatly in diffusing or disseminating knowledge, maybe it's not worth you pursuing it at all.

If a well-stocked lab and a few specialists could have banged this out in a week (which they could), was it worth it to bang this out in a year? Did anything actually go wrong, or is this just how it works?

It's interesting thoughts to toy with, but probably not a great idea to get wrapped up in this sort of self-pity and melancholic thinking. Or maybe it is. Who knows.

Doing the same thing that I was doing before - sitting in a basement, working on electronics. What are the odds that the thing that I was doing before was the right thing to do against a virus?

\rule{\linewidth}{0.2pt}

When to publish?

One the one hand, there're some pretty good reasons to want embargoes, like the Ingelfinger rules - no need to lather up a furor just for some idle musings, and publishing too early is a terrible idea. In retrospect, it was extremely good that I did not make a fuss about (or obtain funding for the original ) some extremely positive data; it turned out to be an embarrassing mistake in a data analysis script.

On the other hand, embargo rules came from publishers, and (like ) these decisions were probably somewhat motivated by financial considerations.

There's a recent movement towards more "live" and open science - it's hard to tell when the, especially in a pandemic. Fortunately, it's not like any of what we're discussing here actually matters, anyways.

\rule{\linewidth}{0.2pt}

This one was taught to me by Yuriy!

We were trying to write a pre-baked math worksheet into Wolfram Mathematica.

It is useful to first strictly implement a method precisely as written, and only when all parts are operational and understood, begin modifying for your own use.

This is common practice in many fields, but hadn't occurred to me for math. 

\rule{\linewidth}{0.2pt}

Indexes of papers compiled by government agencies, and gray literature in general, appear to be very useful.

\rule{\linewidth}{0.2pt}

Rigorous literature reviews are very important. de Seze has a good example: search terms are specified. If we had come across the paper by Bigior earlier, we may have

\rule{\linewidth}{0.2pt}

Papers only carry the scientific heritage that their authors saw fit to include in the references section, and only provide the context included in the "introduction". 

\rule{\linewidth}{0.2pt}

There seems to be a drive towards small volumes and microfluidics. This is great; these are cool technologies; but in our case these were a huge hindrance.

There are a lot of reasons why small volumes are useful: you can run more tests with a given quantity of sample - a 1500-well microplate assay would be a hard sell with even 1.5 mL tubes; small samples are diluted to a lesser degree. 

However, it should also be considered that most tolerances and precisions do not scale with volume. 
Rheology does not scale with volume, so bacterial culture becomes a challenge. Dust and contaminants do not scale. Optical path lengths are much shorter. Most assays will have decreased resolution. Precision in dispensing is much more critical, as is deformation due to autoclaving. The "hairs" that appear on machined polypropylene, which are almost impossible to remove, can be as large as your sample. Evaporation is much more critical.

Most importantly, annoyance scales inversely with size. It is hard to work with a quantity of fluid which is barely visible.

\rule{\linewidth}{0.2pt}

We often came across a situation in which minor errors in software,  would completely change the conclusion of our results.

Reproducible coding helps, but it would be much better if we had used standard software engineering best practices; red-teaming, unit testing on randomly generated data, etc. It might also be helpful if the data itself are blinded against randomly generated controls - in many cases that's not possible, but it might be useful in some circumstances.

See Phosphine.

It may be useful to make explicit to the reader where particular sensitivities - high gradients of conclusion against some factor - might exist in your research.

\rule{\linewidth}{0.2pt}

The phage field - and many other fields - seem to have a problem where most of the really important base discoveries related to practice - 

Some things actually seem to have been lost.

\rule{\linewidth}{0.2pt}

Writing a paper by writing the chapters and headings that you want, and then filling in the text, is quite hard. Just writing a big chunk of text, and then breaking that into many smaller sections, works a bit better.


\rule{\linewidth}{0.2pt}

It might be useful to watch file changes and autoreload. Practically every program in this project required the same three keystrokes to recompile; a recipe for RSI.

\rule{\linewidth}{0.2pt}

Automatic, no-effort documentation of simulation and experiment runs is critical.

It may be helpful to think of simulations very similarly to IRL experiments. 

A simulation is just a universe in a bottle that you can examine more closely; as with real-world, you can learn much from observation, but 

But this need for documentation conflicts with the rapid, iterative cycle necessary for productivity.

This is obvious to all competent, but when rapidly iteratively testing with simulations, it may be helpful to automatically save a package with images of all the components (schematics, graphs, input files) of each distinct test. 

For comparison to experiment, having a webcam take an image of the assembled board is also helpful. 

Many months of testing have vanished into the ether due to 

Version control alone isn't quite enough. Just having a simulation setup file somewhere in the commit history isn't "discoverable" - that is, you must be able to see what the input and output was without re-running the simulation. 

Manually taking notes tended to disrupt the flow of testing; and in any case, just noting "SIR filter has appropriate phase response" is almost useless. What {\it was} the phase response? Plot it, and upload it!

Software such as Sumatra, Sacred, recipy, and others. In our case, we used eLabFTW's elabapy bindings.

I think this is a good general rule: record as much detail as automatically as possible

\rule{\linewidth}{0.2pt}

It is far easier to use existing, well-characterized reference designs verbatim than to modify 

\rule{\linewidth}{0.2pt}

Dill

When rapidly testing data analyses of simulations which aren't really conducive to unit testing, I previously used to run the whole simulation, look at the analysis, re-run the sim, etc.

This is slow. I often don't change the input parameters for the sim, but just the analysis. However, there's often a large amount of state to persist to disk, so it's not worth writing a serializer just to store this throwaway run.

In this case, saving the entire simulation session with 'dill' is very helpful.

\rule{\linewidth}{0.2pt}

Software opacity is evil.

\rule{\linewidth}{0.2pt}

A great deal of time was spent trying to resolve version conflicts and dependency hells with the numerous libraries used by all the simulation programs. Over a week was spent trying to recursively track down all the This also wastes developer time - some fraction of issues raised are due to library version conflicts. 

Packaged binaries help this slightly, but of course don't help if modifications are required, and managing shared libraries is still a tricky matter.

Good solutions include OpenFOAM's Docker installation. In some cases, using chroot with the original developer's Linux distribution is also of some utility.

But this all seems like quite a lot of overhead and opacity for what ultimately doesn't seem a super-complex problem: deterministically obtain a known-good version of a library, build locally, and set paths appropriately.

A good example might be gTest's cmake integration.

In the extreme, systems exist to extract every 

In some situations (especially where the library has a permissive licence) perhaps it could be useful to consider packaging a complete, batteries-included 'known good' repository, either with the source of all the correct library versions included, or with a script to clone and compile the specific version used, integrating all the libraries with the build system. 

For instance, this was done with the PDB reader in the OpenMM wrapper and JAMES.

Honestly, this might already account for a difference in productivity between, say, Python, and C++. the package manager.

\rule{\linewidth}{0.2pt}

There's also a very neat thing that seems to be common in computational biology, but which doesn't seem popular in other fields. After an algorithm or software tool is written (and the source published separately), a simple CGI frontend is written around it and hosted on the university's servers. eLNemo and the CHARMMing web interface are advanced examples of this. This way, anyone with an input file can get results without futzing around with installation, at least for the few decades that the servers are maintained.

The computational biologists have beat us at our own game.

\rule{\linewidth}{0.2pt}

We have always encountered wasting extreme amounts of time on subtle assembly mistakes in hardware prototypes. 

In one example in this project, many hours were wasted because the enamel insulation on a bodge wire had not burned off completely in a solder joint, leading to a high-impedance connection.

The same has occurred in previous projects; in one example, many days were spent debugging software to fix an apparently slow hardware interrupt, which ended up being the result of a poor solder paste stencil leading to a hidden high-impedance connection to a leadless package.

Many failures are the result of carelessness in modification and lack of inspection. Others would only have been found by a 100\% electrical test.

If flying-probe or bed-of-nails tests can be made sufficiently rapid and closely coupled with the existing toolchain, 

In {\it 2001: A space odyssey}, an automated system is shown guiding the troubleshooting of an assembly, apparently generating a fault tree of all the 

Boundary-scan features might really help 

\rule{\linewidth}{0.2pt}

Numerology doesn't pay

\rule{\linewidth}{0.2pt}

The ability to almost immediately compare simulation to experiment was of great use almost everywhere in this project. This hybrid, "lockstep" approach between theory, simulation, and practice works, I think, extremely well. Even if all these intermediate experiments fail, the mere process of setting up an experiment offers you insight - and forces you to persue leads - which would not otherwise have been gained. Is it better than, say, spending a year carefully establishing the theory behind an experiment, and then doing one? I'm not sure. Other teams have had great success with this.

\rule{\linewidth}{0.2pt}

This project started at the tail end of a project to produce ceramics. Many weeks were wasted; I had convinced myself that ceramics were required over FR4. 

\rule{\linewidth}{0.2pt}

I originally rationalized not contacting others about . There are some 10,000 labs working on a cure; it's not helpful to have random amateurs tying up expert time with ignorant flights of fancy. 

Later, as the project became more mature and seemed likely, this morphed into a sort of arrogance.

An expert in microwave engineering could probably have completed this entire project in a few days, rather than the months it took us.

\rule{\linewidth}{0.2pt}

\subsection{Hall of Hubris} \

Lest our hats stop fitting - a reminder that arrogance, fallacious beliefs, etc can creep in at any point

\rule{\linewidth}{0.2pt}

An inane remark:

\begin{displayquote}
We believe once we have the P. Syringae host, we from environmental samples
\end{displayquote}

Truly the depths of Dunning-Kruger.




\rule{\linewidth}{0.2pt}

Sometimes vertical integration is the wrong move. Objectively, in this case, it was simply the *wrong decision*. 

\rule{\linewidth}{0.2pt}


An expert and distinguished gentleman that we contacted regarding assistance resolving transients in our microstrip VCO stated the following:



This was a perfectly sensible remark; it is almost always the case that (at X-band, no less!) a custom IC would have been needed to build a VCO.

Also, if taken in the context of a tired, overworked PI getting an unsolicited email from an excessively verbose undergraduate at a different university, I hardly think I would have replied differently.

However, I think this person may have missed out. Learning 

So perhaps it is wise to ponder the ideas of fools for skeet; one always learns target practice, if only an example what not to do, and occasionally one learns positively. I have often found that I learn greatly from working on the projects of others.

We present this only as a cautionary tale in the hopes that someday I will listen.

 This is 

but, some months later, it was found that he was *right* - absolutely right. This was a grave error in management!

\rule{\linewidth}{0.2pt}

Photomultiplier

"Does anyone know of a circuit that can discriminate color PMT"?

only works if scintillator


%\end{multicols}

\end{document}